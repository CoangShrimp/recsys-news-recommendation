import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import pandas as pd
import numpy as np
import os
import zipfile

# Import c√°c module c·ªßa b·∫°n (ƒë·∫£m b·∫£o file preprocess.py v√† model.py n·∫±m c√πng th∆∞ m·ª•c)
import preprocess as pp
from model import MINDRecModel

# ==========================================
# 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (CONFIG)
# ==========================================
# File zip c·∫ßn d·ª± ƒëo√°n (Dev set)
ZIP_TEST_PATH = 'MINDlarge_dev.zip'
# Th∆∞ m·ª•c s·∫Ω gi·∫£i n√©n ra
DIR_TEST_EXTRACTED = './mind_large_dev_data'

# ƒê∆∞·ªùng d·∫´n t·∫≠p Train c≈© (D√πng ƒë·ªÉ l·∫•y l·∫°i b·ªô t·ª´ ƒëi·ªÉn - B·∫ÆT BU·ªòC)
# N·∫øu b·∫°n l∆∞u t√™n kh√°c, h√£y s·ª≠a d√≤ng n√†y
DIR_TRAIN_DATA = 'MIND_small_train' 

# ƒê∆∞·ªùng d·∫´n model ƒë√£ train
MODEL_PATH = 'checkpoints/mind_model.pth'
# File k·∫øt qu·∫£ ƒë·∫ßu ra
OUTPUT_PATH = 'prediction.txt'

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# 2. H√ÄM GI·∫¢I N√âN (T·ª± ƒë·ªông ch·∫°y)
# ==========================================
def extract_data_if_needed():
    # Ki·ªÉm tra xem folder ƒë√£ t·ªìn t·∫°i v√† c√≥ file ch∆∞a
    if os.path.exists(DIR_TEST_EXTRACTED) and os.path.exists(os.path.join(DIR_TEST_EXTRACTED, 'news.tsv')):
        print(f"‚úÖ ƒê√£ t√¨m th·∫•y d·ªØ li·ªáu t·∫°i {DIR_TEST_EXTRACTED}. B·ªè qua gi·∫£i n√©n.")
        return

    print(f"üì¶ ƒêang gi·∫£i n√©n {ZIP_TEST_PATH}...")
    if not os.path.exists(ZIP_TEST_PATH):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file zip: {ZIP_TEST_PATH}")
        
    try:
        with zipfile.ZipFile(ZIP_TEST_PATH, 'r') as zip_ref:
            zip_ref.extractall(DIR_TEST_EXTRACTED)
        print(f"‚úÖ Gi·∫£i n√©n th√†nh c√¥ng v√†o: {DIR_TEST_EXTRACTED}")
    except Exception as e:
        print(f"‚ùå L·ªói gi·∫£i n√©n: {e}")
        raise

# ==========================================
# 3. LOGIC D·ª∞ ƒêO√ÅN (PREDICT)
# ==========================================
def predict_one_user(model, history_str, impressions_str, news_title_matrix):
    """
    H√†m t√≠nh to√°n ƒëi·ªÉm cho 1 user
    """
    # A. X·ª≠ l√Ω History (L·ªãch s·ª≠ ƒë·ªçc)
    history_ids = [] if pd.isna(history_str) else str(history_str).split(' ')
    # C·∫Øt ho·∫∑c pad history cho ƒë√∫ng chi·ªÅu d√†i quy ƒë·ªãnh
    if len(history_ids) > pp.MAX_HISTORY_LENGTH: 
        history_ids = history_ids[-pp.MAX_HISTORY_LENGTH:]
    
    # Map NewsID -> Vector s·ªë (Sequence)
    history_seqs = [news_title_matrix.get(nid, [0]*pp.MAX_TITLE_LENGTH) for nid in history_ids]
    # Padding n·∫øu history ng·∫Øn qu√°
    while len(history_seqs) < pp.MAX_HISTORY_LENGTH:
        history_seqs.insert(0, [0]*pp.MAX_TITLE_LENGTH)
    
    # B. X·ª≠ l√Ω Candidate List (Danh s√°ch b√†i c·∫ßn x·∫øp h·∫°ng)
    candidates = []
    impression_items = impressions_str.split(' ')
    
    for item in impression_items:
        # item d·∫°ng "N12345-0" ho·∫∑c "N12345-1". Ta c·∫Øt l·∫•y ID "N12345"
        nid = item.split('-')[0]
        candidates.append(news_title_matrix.get(nid, [0]*pp.MAX_TITLE_LENGTH))
        
    # C. Chuy·ªÉn th√†nh Tensor ƒë·ªÉ ƒë∆∞a v√†o GPU/CPU
    # History: [Batch=1, Max_History_Len, Title_Len] -> v√≠ d·ª• [1, 50, 30]
    history_tensor = torch.tensor([history_seqs], dtype=torch.long).to(DEVICE)
    # Candidate: [Num_Candidates, Title_Len] -> v√≠ d·ª• [N, 30]
    candidate_tensor = torch.tensor(candidates, dtype=torch.long).to(DEVICE)
    
    # D. Ch·∫°y qua Model (Inference)
    with torch.no_grad():
        # 1. M√£ h√≥a ng∆∞·ªùi d√πng
        user_vector = model.user_encoder(history_tensor) # [1, 400]
        
        # 2. M√£ h√≥a c√°c b√†i b√°o ·ª©ng vi√™n
        news_vectors = model.news_encoder(candidate_tensor) # [N, 400]
        
        # 3. T√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng (Dot Product)
        # K·∫øt qu·∫£: [N] ƒëi·ªÉm s·ªë
        scores = torch.matmul(user_vector, news_vectors.t()).squeeze()
        
        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ch·ªâ c√≥ 1 candidate (squeeze l√†m m·∫•t chi·ªÅu)
        if scores.ndim == 0:
            scores = scores.unsqueeze(0)
            
    return scores.cpu().numpy().tolist()

def main():
    # --- B∆∞·ªõc 0: Gi·∫£i n√©n d·ªØ li·ªáu ---
    extract_data_if_needed()

    # --- B∆∞·ªõc 1: T√°i t·∫°o t·ª´ ƒëi·ªÉn (Vocab) t·ª´ t·∫≠p TRAIN ---
    # C·∫¢NH B√ÅO: Ph·∫£i d√πng t·∫≠p TRAIN ƒë·ªÉ build vocab, kh√¥ng d√πng t·∫≠p DEV/TEST.
    print(f"üìñ ƒêang t√°i t·∫°o t·ª´ ƒëi·ªÉn t·ª´ {DIR_TRAIN_DATA}...")
    if not os.path.exists(os.path.join(DIR_TRAIN_DATA, 'news.tsv')):
         raise FileNotFoundError(f"‚ùå C·∫ßn th∆∞ m·ª•c {DIR_TRAIN_DATA} ƒë·ªÉ l·∫•y l·∫°i b·ªô t·ª´ ƒëi·ªÉn c≈©. H√£y gi·∫£i n√©n MINDsmall_train v√†o ƒë√¢y.")

    df_news_train = pp.load_news_data(os.path.join(DIR_TRAIN_DATA, 'news.tsv'))
    word2index = pp.build_vocab(df_news_train['title'])
    vocab_size = len(word2index) + 1
    print(f"‚úÖ K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn: {vocab_size} t·ª´.")

    # --- B∆∞·ªõc 2: Load d·ªØ li·ªáu DEV (Large) ---
    print(f"üì• ƒêang load d·ªØ li·ªáu d·ª± ƒëo√°n t·ª´ {DIR_TEST_EXTRACTED}...")
    # Load news.tsv c·ªßa t·∫≠p Dev
    df_news_dev = pp.load_news_data(os.path.join(DIR_TEST_EXTRACTED, 'news.tsv'))
    # Load behaviors.tsv c·ªßa t·∫≠p Dev
    df_behaviors_dev = pp.load_behaviors_data(os.path.join(DIR_TEST_EXTRACTED, 'behaviors.tsv'))

    # Cache (l∆∞u ƒë·ªám) ti√™u ƒë·ªÅ b√†i b√°o Dev th√†nh c√°c con s·ªë
    print("‚è≥ ƒêang m√£ h√≥a ti√™u ƒë·ªÅ b√†i b√°o Dev...")
    news_title_matrix = {}
    # K·∫øt h·ª£p c·∫£ news train (ph√≤ng h·ªù history c≈©) v√† news dev
    # ∆Øu ti√™n News Dev n·∫øu tr√πng ID
    for news_id, row in tqdm(df_news_dev.iterrows(), total=len(df_news_dev)):
        news_title_matrix[news_id] = pp.transform_text(row['title'], word2index)
    
    # --- B∆∞·ªõc 3: Load Model ---
    print(f"ü§ñ ƒêang load model t·ª´ {MODEL_PATH}...")
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file model: {MODEL_PATH}")

    model = MINDRecModel(num_words=vocab_size).to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval() # Ch·∫ø ƒë·ªô ƒë√°nh gi√° (t·∫Øt Dropout)

    # --- B∆∞·ªõc 4: Ch·∫°y d·ª± ƒëo√°n ---
    print(f"üöÄ B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n cho {len(df_behaviors_dev)} d√≤ng log...")
    
    with open(OUTPUT_PATH, 'w') as f:
        for idx, row in tqdm(df_behaviors_dev.iterrows(), total=len(df_behaviors_dev)):
            impression_id = row['impression_id']
            history_str = row['history']
            impressions_str = row['impressions']
            
            try:
                # T√≠nh ƒëi·ªÉm
                scores = predict_one_user(model, history_str, impressions_str, news_title_matrix)
                
                # Chuy·ªÉn ƒëi·ªÉm th√†nh Rank (Th·ª© h·∫°ng)
                # argsort(-scores) -> s·∫Øp x·∫øp index theo ƒëi·ªÉm gi·∫£m d·∫ßn
                # argsort l·∫ßn n·ªØa -> l·∫•y th·ª© h·∫°ng
                ranks = (np.argsort(np.argsort(-np.array(scores))) + 1).tolist()
                
                # Format: ID [rank1,rank2,...]
                rank_str = '[' + ','.join(map(str, ranks)) + ']'
                f.write(f"{impression_id} {rank_str}\n")
            except Exception as e:
                # N·∫øu l·ªói d√≤ng n√†o th√¨ ghi log v√† b·ªè qua ƒë·ªÉ kh√¥ng ch·∫øt ch∆∞∆°ng tr√¨nh
                print(f"‚ö†Ô∏è L·ªói t·∫°i impression {impression_id}: {e}")

    print(f"\nüéâ XONG! File k·∫øt qu·∫£ l∆∞u t·∫°i: {OUTPUT_PATH}")
    print("M·∫πo: N√©n file n√†y th√†nh zip v√† n·ªôp l√™n h·ªá th·ªëng ch·∫•m ƒëi·ªÉm.")

if __name__ == "__main__":
    main()