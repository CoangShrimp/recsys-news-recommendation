import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import pandas as pd
import numpy as np
import os
import zipfile

# Import module n·ªôi b·ªô
import preprocess as pp
from model import MINDRecModel

# ==========================================
# 1. C·∫§U H√åNH (CONFIG)
# ==========================================
# T√™n file zip (ch·ªâ d√πng n·∫øu ch∆∞a gi·∫£i n√©n)
ZIP_TEST_PATH = 'MINDlarge_dev.zip' 

# Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu ƒë·∫ßu v√†o (Quan tr·ªçng: Run_Test_Set.py s·∫Ω ƒë·ªï d·ªØ li·ªáu v√†o ƒë√¢y)
DIR_TEST_EXTRACTED = './mind_large_dev_data'

# Th∆∞ m·ª•c Train c≈© (ƒë·ªÉ l·∫•y b·ªô t·ª´ ƒëi·ªÉn Word2Index)
DIR_TRAIN_DATA = 'MIND_small_train' 

# ƒê∆∞·ªùng d·∫´n Model checkpoint
MODEL_PATH = 'checkpoints/mind_model.pth'

# File k·∫øt qu·∫£
OUTPUT_PATH = 'prediction.txt'

# Thi·∫øt b·ªã (GPU/CPU)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# 2. H√ÄM GI·∫¢I N√âN (An to√†n)
# ==========================================
def extract_data_if_needed():
    """
    Ch·ªâ gi·∫£i n√©n n·∫øu ch∆∞a c√≥ d·ªØ li·ªáu. 
    Kh√¥ng crash n·∫øu thi·∫øu file zip nh∆∞ng d·ªØ li·ªáu ƒë√£ c√≥ s·∫µn.
    """
    # 1. Ki·ªÉm tra n·∫øu d·ªØ li·ªáu ƒë√£ t·ªìn t·∫°i (do Run_Test_Set.py t·∫°o)
    if os.path.exists(DIR_TEST_EXTRACTED) and os.path.exists(os.path.join(DIR_TEST_EXTRACTED, 'news.tsv')):
        print(f"‚úÖ ƒê√£ t√¨m th·∫•y d·ªØ li·ªáu t·∫°i '{DIR_TEST_EXTRACTED}'. S·∫µn s√†ng d·ª± ƒëo√°n!")
        return

    # 2. N·∫øu ch∆∞a c√≥ d·ªØ li·ªáu, th·ª≠ gi·∫£i n√©n
    print(f"üì¶ Kh√¥ng th·∫•y folder d·ªØ li·ªáu, ƒëang th·ª≠ t√¨m {ZIP_TEST_PATH}...")
    if not os.path.exists(ZIP_TEST_PATH):
        # N·∫øu kh√¥ng c√≥ zip c≈©ng kh√¥ng c√≥ folder -> L·ªói
        print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file zip '{ZIP_TEST_PATH}' v√† c≈©ng kh√¥ng c√≥ folder '{DIR_TEST_EXTRACTED}'.")
        print("üëâ N·∫øu b·∫°n ƒëang ch·∫°y Test Set t·ª´ Drive, h√£y ƒë·∫£m b·∫£o Run_Test_Set.py ƒë√£ ch·∫°y th√†nh c√¥ng tr∆∞·ªõc b∆∞·ªõc n√†y.")
        # Kh√¥ng raise l·ªói ngay, ƒë·ªÉ code d∆∞·ªõi th·ª≠ load file r·ªìi m·ªõi b√°o l·ªói chi ti·∫øt
        return
        
    try:
        with zipfile.ZipFile(ZIP_TEST_PATH, 'r') as zip_ref:
            zip_ref.extractall(DIR_TEST_EXTRACTED)
        print(f"‚úÖ Gi·∫£i n√©n th√†nh c√¥ng v√†o: {DIR_TEST_EXTRACTED}")
    except Exception as e:
        print(f"‚ùå L·ªói gi·∫£i n√©n: {e}")

# ==========================================
# 3. LOGIC D·ª∞ ƒêO√ÅN (ƒê√£ s·ª≠a l·ªói Strip)
# ==========================================
def predict_one_user(model, history_str, impressions_str, news_title_matrix):
    """
    D·ª± ƒëo√°n ƒëi·ªÉm cho 1 user.
    """
    # --- A. X·ª≠ l√Ω History ---
    # FIX: Th√™m .strip() ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    if pd.isna(history_str):
        history_ids = []
    else:
        history_ids = str(history_str).strip().split(' ')
        
    # L·∫•y 50 b√†i g·∫ßn nh·∫•t
    if len(history_ids) > pp.MAX_HISTORY_LENGTH: 
        history_ids = history_ids[-pp.MAX_HISTORY_LENGTH:]
    
    # Map ID sang Vector
    # N·∫øu ID kh√¥ng c√≥ trong dict (b√†i m·ªõi), d√πng vector 0
    history_seqs = [news_title_matrix.get(nid, [0]*pp.MAX_TITLE_LENGTH) for nid in history_ids]
    
    # N·∫øu history r·ªóng (User m·ªõi), th√™m √≠t nh·∫•t 1 vector 0 ƒë·ªÉ kh√¥ng b·ªã l·ªói dimention
    if not history_seqs:
        history_seqs.append([0]*pp.MAX_TITLE_LENGTH)

    # Padding v·ªÅ ƒë·ªô d√†i chu·∫©n (50)
    while len(history_seqs) < pp.MAX_HISTORY_LENGTH:
        history_seqs.insert(0, [0]*pp.MAX_TITLE_LENGTH)
    
    # --- B. X·ª≠ l√Ω Candidate ---
    candidates = []
    # FIX: Th√™m .strip() c·ª±c k·ª≥ quan tr·ªçng ƒë·ªÉ tr√°nh t·∫°o ra ph·∫ßn t·ª≠ r·ªóng ''
    impression_items = str(impressions_str).strip().split(' ')
    
    for item in impression_items:
        if not item: continue # B·ªè qua n·∫øu c√≥ item r·ªóng
        # item c√≥ th·ªÉ l√† "N12345-0" (Dev) ho·∫∑c "N12345" (Test)
        # split('-')[0] x·ª≠ l√Ω ƒë∆∞·ª£c c·∫£ 2 tr∆∞·ªùng h·ª£p
        nid = item.split('-')[0]
        candidates.append(news_title_matrix.get(nid, [0]*pp.MAX_TITLE_LENGTH))
        
    # --- C. Tensor & Inference ---
    # History: [1, 50, 30]
    history_tensor = torch.tensor([history_seqs], dtype=torch.long).to(DEVICE)
    # Candidate: [N, 30]
    candidate_tensor = torch.tensor(candidates, dtype=torch.long).to(DEVICE)
    
    with torch.no_grad():
        user_vector = model.user_encoder(history_tensor) # [1, 400]
        news_vectors = model.news_encoder(candidate_tensor) # [N, 400]
        
        # Dot product: [1, 400] x [400, N] -> [1, N] -> squeeze -> [N]
        scores = torch.matmul(user_vector, news_vectors.t()).squeeze()
        
        # N·∫øu ch·ªâ c√≥ 1 candidate, squeeze l√†m n√≥ th√†nh scalar (0-d), c·∫ßn unsqueeze l·∫°i th√†nh 1-d array
        if scores.ndim == 0:
            scores = scores.unsqueeze(0)
            
    return scores.cpu().numpy().tolist()

def main():
    # --- B∆∞·ªõc 0: Chu·∫©n b·ªã d·ªØ li·ªáu ---
    extract_data_if_needed()

    # --- B∆∞·ªõc 1: Build Vocab (T·ª´ t·∫≠p Train g·ªëc) ---
    print(f"üìñ ƒêang load t·ª´ ƒëi·ªÉn t·ª´ {DIR_TRAIN_DATA}...")
    if not os.path.exists(os.path.join(DIR_TRAIN_DATA, 'news.tsv')):
         raise FileNotFoundError(f"‚ùå C·∫ßn th∆∞ m·ª•c '{DIR_TRAIN_DATA}' ch·ª©a news.tsv (MINDsmall_train) ƒë·ªÉ t√°i t·∫°o b·ªô t·ª´ ƒëi·ªÉn.")

    df_news_train = pp.load_news_data(os.path.join(DIR_TRAIN_DATA, 'news.tsv'))
    word2index = pp.build_vocab(df_news_train['title'])
    vocab_size = len(word2index) + 1
    print(f"‚úÖ Vocab size: {vocab_size}")

    # --- B∆∞·ªõc 2: Load Data c·∫ßn d·ª± ƒëo√°n ---
    print(f"üì• ƒêang ƒë·ªçc d·ªØ li·ªáu d·ª± ƒëo√°n t·ª´ '{DIR_TEST_EXTRACTED}'...")
    news_path = os.path.join(DIR_TEST_EXTRACTED, 'news.tsv')
    behaviors_path = os.path.join(DIR_TEST_EXTRACTED, 'behaviors.tsv')
    
    if not os.path.exists(news_path) or not os.path.exists(behaviors_path):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file data trong {DIR_TEST_EXTRACTED}. H√£y ki·ªÉm tra l·∫°i b∆∞·ªõc gi·∫£i n√©n.")

    df_news_dev = pp.load_news_data(news_path)
    df_behaviors_dev = pp.load_behaviors_data(behaviors_path)
    print(f"   + S·ªë l∆∞·ª£ng b√†i b√°o: {len(df_news_dev)}")
    print(f"   + S·ªë l∆∞·ª£ng logs c·∫ßn d·ª± ƒëo√°n: {len(df_behaviors_dev)}")

    # M√£ h√≥a ti√™u ƒë·ªÅ b√†i b√°o (Cache)
    print("‚è≥ ƒêang m√£ h√≥a ti√™u ƒë·ªÅ b√†i b√°o (Embedding lookup)...")
    news_title_matrix = {}
    for news_id, row in tqdm(df_news_dev.iterrows(), total=len(df_news_dev)):
        news_title_matrix[news_id] = pp.transform_text(row['title'], word2index)
    
    # --- B∆∞·ªõc 3: Load Model ---
    print(f"ü§ñ ƒêang load model: {MODEL_PATH}...")
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y model t·∫°i {MODEL_PATH}")

    model = MINDRecModel(num_words=vocab_size).to(DEVICE)
    # map_location ƒë·∫£m b·∫£o load ƒë∆∞·ª£c tr√™n c·∫£ CPU n·∫øu train b·∫±ng GPU
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()

    # --- B∆∞·ªõc 4: Ch·∫°y d·ª± ƒëo√°n ---
    print("üöÄ B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n...")
    
    with open(OUTPUT_PATH, 'w') as f:
        for idx, row in tqdm(df_behaviors_dev.iterrows(), total=len(df_behaviors_dev)):
            impression_id = row['impression_id']
            history_str = row['history']
            impressions_str = row['impressions']
            
            try:
                # L·∫•y ƒëi·ªÉm s·ªë
                scores = predict_one_user(model, history_str, impressions_str, news_title_matrix)
                
                # Chuy·ªÉn th√†nh Rank (1 l√† cao nh·∫•t)
                # argsort(-scores) tr·∫£ v·ªÅ index c·ªßa ph·∫ßn t·ª≠ l·ªõn nh·∫•t ƒë·∫øn nh·ªè nh·∫•t
                sorted_indices = np.argsort(-np.array(scores))
                
                # Rank th·ª±c t·∫ø l√† v·ªã tr√≠ trong m·∫£ng ƒë√£ sort + 1? 
                # KH√îNG, MIND y√™u c·∫ßu rank c·ªßa t·ª´ng b√†i theo th·ª© t·ª± ban ƒë·∫ßu.
                # V√≠ d·ª• input: [Item1, Item2] -> scores: [0.9, 0.1] -> Output: [1, 2]
                # V√≠ d·ª• input: [Item1, Item2] -> scores: [0.1, 0.9] -> Output: [2, 1]
                
                # C√°ch t·∫°o rank chu·∫©n format MIND:
                # Ta c·∫ßn x·∫øp h·∫°ng cho t·ª´ng v·ªã tr√≠.
                # ranks[i] = th·ª© h·∫°ng c·ªßa item i
                
                # D√πng scipy.stats.rankdata ho·∫∑c logic ƒë·∫£o ng∆∞·ª£c argsort
                n = len(scores)
                ranks = [0] * n
                for rank, index in enumerate(sorted_indices):
                    ranks[index] = rank + 1
                
                # Format: [rank1,rank2,...]
                rank_str = '[' + ','.join(map(str, ranks)) + ']'
                f.write(f"{impression_id} {rank_str}\n")
                
            except Exception as e:
                print(f"\n‚ö†Ô∏è L·ªói Impression {impression_id}: {e}")
                # Fallback: ƒëi·ªÅn random ranks ƒë·ªÉ kh√¥ng b·ªã ch·∫øt ch∆∞∆°ng tr√¨nh
                # (Quan tr·ªçng ƒë·ªÉ file output v·∫´n ƒë·ªß d√≤ng)
                try:
                    count = len(str(impressions_str).strip().split(' '))
                    fallback_ranks = list(range(1, count + 1))
                    f.write(f"{impression_id} {'[' + ','.join(map(str, fallback_ranks)) + ']'}\n")
                except:
                    pass

    print(f"\nüéâ HO√ÄN T·∫§T! K·∫øt qu·∫£ l∆∞u t·∫°i: {os.path.abspath(OUTPUT_PATH)}")

if __name__ == "__main__":
    main()